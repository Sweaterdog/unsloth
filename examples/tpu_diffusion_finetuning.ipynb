{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HXDgQ_gayHGl",
      "metadata": {
        "id": "HXDgQ_gayHGl"
      },
      "source": [
        "# Fine-tuning Stable Diffusion with LoRA on TPUs using Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XnUSUdTqyHGm",
      "metadata": {
        "id": "XnUSUdTqyHGm"
      },
      "source": [
        "This notebook demonstrates how to fine-tune a pre-trained Stable Diffusion model on a specific task using LoRA (Low-Rank Adaptation) with Unsloth's `FastDiffusionModel` and `DiffusionTrainer` on a TPU runtime.\n",
        "\n",
        "**Key Steps:**\n",
        "1. Setup: Install libraries and configure TPU environment.\n",
        "2. Load Model: Use `FastDiffusionModel` to load a Stable Diffusion pipeline.\n",
        "3. Prepare Dataset: Create or load an image-caption dataset and preprocess it.\n",
        "4. Configure LoRA: Apply LoRA adapters to the UNet component of the diffusion model.\n",
        "5. Define Training Function: Create a function for the distributed training loop.\n",
        "6. Launch Distributed Training: Use `DiffusionTrainer.launch_distributed` to train on multiple TPU cores.\n",
        "7. Inference: Load the fine-tuned LoRA adapters and generate images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zuiqpc5yHGn",
      "metadata": {
        "id": "2zuiqpc5yHGn"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2XjgbrwLyHGn",
      "metadata": {
        "id": "2XjgbrwLyHGn"
      },
      "source": [
        "**Ensure TPU Runtime:**\n",
        "If you are using Google Colab, make sure to select a TPU runtime:\n",
        "1. Go to `Runtime` -> `Change runtime type`.\n",
        "2. Select `TPU` from the `Hardware accelerator` dropdown.\n",
        "\n",
        "**Install Libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "T1Vde2sHyHGo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T1Vde2sHyHGo",
        "outputId": "6b2004cc-6f27-4b7e-d611-d89885933e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.15.2\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting triton\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.6.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting dill\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.6.2-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, xxhash, xformers, unsloth_zoo, trl, peft, multiprocess, hf_transfer, dill, diffusers, datasets, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.33.1 dill-0.4.0 hf_transfer-0.1.9 multiprocess-0.70.18 peft-0.15.2 triton-3.3.1 trl-0.15.2 unsloth_zoo-2025.6.2 xformers-0.0.29.post3 xxhash-3.5.0\n",
            "Collecting unsloth@ git+https://github.com/Sweaterdog/unsloth.git (from unsloth[tpu]@ git+https://github.com/Sweaterdog/unsloth.git)\n",
            "  Cloning https://github.com/Sweaterdog/unsloth.git to /tmp/pip-install-c9qtsdz_/unsloth_f7bdffcf7e6d4da0a2f03256f842c1c7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Sweaterdog/unsloth.git /tmp/pip-install-c9qtsdz_/unsloth_f7bdffcf7e6d4da0a2f03256f842c1c7\n",
            "  Resolved https://github.com/Sweaterdog/unsloth.git to commit 80aa7519aaf3bd2addbe322366058d04c3e11080\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.6.2-py3-none-any.whl size=293134 sha256=2320c441e351f18cced78ed3200fd77bcc9304266030e67a5f3456080149e3d8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c11rt7up/wheels/40/de/f2/f3b767ee2236d00238d4a46893b528634b8620f1f6459485ef\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.6.2\n",
            "Collecting torch_xla\n",
            "  Downloading torch_xla-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (21 kB)\n",
            "Collecting absl-py>=1.0.0 (from torch_xla)\n",
            "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting numpy (from torch_xla)\n",
            "  Downloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml (from torch_xla)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests (from torch_xla)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->torch_xla)\n",
            "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torch_xla)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->torch_xla)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torch_xla)\n",
            "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading torch_xla-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (96.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.7/157.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, pyyaml, numpy, idna, charset_normalizer, certifi, absl-py, requests, torch_xla\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.6.15\n",
            "    Uninstalling certifi-2025.6.15:\n",
            "      Successfully uninstalled certifi-2025.6.15\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: torch_xla\n",
            "    Found existing installation: torch-xla 2.6.0\n",
            "    Uninstalling torch-xla-2.6.0:\n",
            "      Successfully uninstalled torch-xla-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.6.2 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.6.2 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.6.2 requires wheel>=0.42.0, which is not installed.\n",
            "unsloth-zoo 2025.6.2 requires protobuf<4.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
            "datasets 3.6.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
            "datasets 3.6.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.3.0 certifi-2025.6.15 charset_normalizer-3.4.2 idna-3.10 numpy-2.3.0 pyyaml-6.0.2 requests-2.32.4 torch_xla-2.7.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy"
                ]
              },
              "id": "a98b6812e6b746fabb2cda870964fc7d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --no-deps diffusers Pillow transformers bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo sentencepiece protobuf datasets huggingface_hub hf_transfer multiprocess xxhash dill\n",
        "!pip install --no-deps \"unsloth[tpu] @ git+https://github.com/Sweaterdog/unsloth.git\" # Install Unsloth with TPU extras from Sweaterdog's fork\n",
        "!pip install --force-reinstall torch_xla\n",
        "\n",
        "# Verify torch_xla installation (optional)\n",
        "try:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"Successfully imported torch_xla.core.xla_model as xm\")\n",
        "    print(f\"Default XLA device: {xm.xla_device()}\")\n",
        "    print(f\"XLA world size: {xm.xrt_world_size()}\")\n",
        "except ImportError:\n",
        "    print(\"torch_xla not found. Please ensure it's installed correctly for TPU usage.\")\n",
        "    print(\"Attempting to install torch_xla\")\n",
        "    !pip install --force-reinstall torch_xla"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_restart_runtime_cell",
      "metadata": {
        "id": "new_restart_runtime_cell"
      },
      "source": [
        "**IMPORTANT: Restart Your Runtime!**\n",
        "\n",
        "After running the package installation cell above, you **MUST** restart the runtime for the changes to the `unsloth` package (especially for TPU device detection) to be loaded correctly.\n",
        "\n",
        "**In Google Colab:**\n",
        "1. Go to the menu: `Runtime` -> `Restart session` (or `Restart runtime`).\n",
        "2. Wait for the session to restart.\n",
        "3. Then, continue running the cells from the **\"Import Libraries:\"** section downwards.\n",
        "\n",
        "Failure to restart may result in a `NotImplementedError` related to device detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AsFaD0gXyHGo",
      "metadata": {
        "id": "AsFaD0gXyHGo"
      },
      "source": [
        "**Import Libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "jGhUOqrVyHGo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jGhUOqrVyHGo",
        "outputId": "a6a5aa70-0fa3-4429-c6df-734161b2310d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1937100216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastDiffusionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusionTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusionTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m[0mcore\u001b[0m\u001b[0;34m.\u001b[0m[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.\"\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=[0m \u001b[0mget_device_type\u001b[0m[0;34m(\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Reduce VRAM usage by reducing fragmentation\u001b[0m\u001b[0;34m\u001b[0m[0;34m\u001b[0m[0m
",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.\"\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=[0m \u001b[0mget_device_type\u001b[0m[0;34m(\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m
",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset # Updated import for load_dataset\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import random\n",
        "\n",
        "from unsloth import FastDiffusionModel, DiffusionTrainer, DiffusionTrainingArguments\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pjU05sRyyHGp",
      "metadata": {
        "id": "pjU05sRyyHGp"
      },
      "source": [
        "## 2. Initialize TPU Distributed Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l6AbjlrJyHGp",
      "metadata": {
        "id": "l6AbjlrJyHGp"
      },
      "source": [
        "TPUs excel at distributed training, where multiple cores work together. Unsloth's `DiffusionTrainer` provides a `launch_distributed` method that simplifies this process. It uses `torch_xla.distributed.xla_multiprocessing.spawn` (xmp.spawn) to run the training function on all available TPU cores.\n",
        "\n",
        "The `train_fn` we define later will encapsulate the training logic for a single process. `launch_distributed` will handle spawning this function across all TPU cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NPQfgYiLyHGp",
      "metadata": {
        "id": "NPQfgYiLyHGp"
      },
      "outputs": [],
      "source": [
        "def check_tpu_availability():\n",
        "    if 'COLAB_TPU_ADDR' in os.environ or 'XRT_TPU_CONFIG' in os.environ:\n",
        "        print(f\"TPU available.\")\n",
        "        try:\n",
        "            print(f\"Number of XLA devices: {xm.xrt_world_size()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not get XLA world size: {e}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"TPU not detected. This notebook is designed for TPU runtimes.\")\n",
        "        print(\"If on Colab, ensure Runtime > Change runtime type > TPU is selected.\")\n",
        "        return False\n",
        "\n",
        "IS_TPU_AVAILABLE = check_tpu_availability()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1xkeVPxRyHGp",
      "metadata": {
        "id": "1xkeVPxRyHGp"
      },
      "source": [
        "## 3. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffFWK6pnyHGp",
      "metadata": {
        "id": "ffFWK6pnyHGp"
      },
      "source": [
        "We'll use `FastDiffusionModel.from_pretrained` to load `stable-diffusion-v1-5/stable-diffusion-v1-5`. This method returns the UNet (as the main `model` object), the tokenizer, and the full diffusers pipeline. The UNet will have references to other components like VAE, text encoder, and scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YGa7GAzWyHGp",
      "metadata": {
        "id": "YGa7GAzWyHGp"
      },
      "outputs": [],
      "source": [
        "model_name = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        "\n",
        "unet, tokenizer, pipeline = FastDiffusionModel.from_pretrained(\n",
        "    model_name_or_path=model_name,\n",
        "    torch_dtype=torch.bfloat16, # bfloat16 is recommended for TPUs\n",
        ")\n",
        "\n",
        "# Access components (they are also attributes of the unet object itself)\n",
        "vae = unet.vae\n",
        "text_encoder = unet.text_encoder\n",
        "scheduler = unet.scheduler\n",
        "\n",
        "print(f\"UNet type: {type(unet)}\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "print(f\"Pipeline type: {type(pipeline)}\")\n",
        "print(f\"VAE type: {type(vae)}\")\n",
        "print(f\"Text Encoder type: {type(text_encoder)}\")\n",
        "print(f\"Scheduler type: {type(scheduler)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48Xdy8zyHGq",
      "metadata": {
        "id": "a48Xdy8zyHGq"
      },
      "source": [
        "## 4. Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4y1OY2yHGq",
      "metadata": {
        "id": "da4y1OY2yHGq"
      },
      "source": [
        "For fine-tuning, we need a dataset of images and corresponding text captions. We will use the `gigant/oldbookillustrations` dataset from Hugging Face Hub. The `DiffusionTrainer` expects inputs in a dictionary format, typically including `pixel_values` (for images) and `input_ids` (for tokenized captions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AyaSnjasyHGq",
      "metadata": {
        "id": "AyaSnjasyHGq"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"gigant/oldbookillustrations\"\n",
        "image_column = \"1600px\" # Default for this dataset\n",
        "caption_column = \"info_alt\" # Default for this dataset, sometimes 'text'\n",
        "\n",
        "image_size = 1600  # runwayml/stable-diffusion-v1-5 uses 512x512 images\n",
        "\n",
        "# Load a subset of the dataset\n",
        "try:\n",
        "    full_dataset = load_dataset(dataset_name)\n",
        "    # Select a small subset for the demo from the 'train' split\n",
        "    # Ensure 'train' split exists, otherwise adapt (e.g. if only 'default' split)\n",
        "    split_name = 'train' if 'train' in full_dataset else list(full_dataset.keys())[0]\n",
        "    hf_dataset = full_dataset[split_name].shuffle(seed=42).select(range(1000))\n",
        "    print(f\"Loaded {len(hf_dataset)} samples from {dataset_name} ({split_name} split).\")\n",
        "    print(f\"Dataset features: {hf_dataset.features}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset {dataset_name}: {e}\")\n",
        "    print(\"Using a dummy dataset as a fallback.\")\n",
        "    num_samples = 64\n",
        "    dataset_dict = {\"image\": [], \"caption\": []}\n",
        "    for i in range(num_samples):\n",
        "        img = Image.new('RGB', (image_size, image_size), color=random.choice([\"red\", \"blue\", \"green\", \"yellow\"]))\n",
        "        dataset_dict[\"image\"].append(img)\n",
        "        dataset_dict[\"caption\"].append(f\"A dummy illustration of a colored shape number {i}\")\n",
        "    hf_dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Preprocessing functions\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "def preprocess_dataset(examples):\n",
        "    images = [image_transforms(image.convert(\"RGB\")) for image in examples[image_column]]\n",
        "    captions = examples[caption_column]\n",
        "\n",
        "    max_len = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length else 77\n",
        "    inputs = tokenizer(\n",
        "        captions, max_length=max_len, padding=\"max_length\",\n",
        "        truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\"pixel_values\": images, \"input_ids\": inputs.input_ids}\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_dataset = hf_dataset.map(\n",
        "    function=preprocess_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=[col for col in hf_dataset.column_names if col not in [\"pixel_values\", \"input_ids\"]], # Keep only needed\n",
        ")\n",
        "\n",
        "# Define the data collator (stacks tensors)\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "print(f\"Processed dataset features: {processed_dataset.features}\")\n",
        "if len(processed_dataset) > 0:\n",
        "    print(f\"Example pixel_values shape: {processed_dataset[0]['pixel_values'].shape}\")\n",
        "    print(f\"Example input_ids shape: {processed_dataset[0]['input_ids'].shape}\")\n",
        "else:\n",
        "    print(\"Processed dataset is empty, check dataset loading and preprocessing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0dj_17yHGq",
      "metadata": {
        "id": "0a0dj_17yHGq"
      },
      "source": [
        "## 5. Configure LoRA and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3vzqZ_k2yHGq",
      "metadata": {
        "id": "3vzqZ_k2yHGq"
      },
      "source": [
        "Now, we'll apply LoRA to the UNet. `FastDiffusionModel.get_peft_model` (which is available on the UNet object itself after loading with `FastDiffusionModel`) helps with this. Then, we define `DiffusionTrainingArguments` and prepare for trainer instantiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t49o3svyyHGq",
      "metadata": {
        "id": "t49o3svyyHGq"
      },
      "outputs": [],
      "source": [
        "# Apply LoRA to the UNet\n",
        "unet_lora = FastDiffusionModel.get_peft_model(\n",
        "    unet,\n",
        "    r=64, # LoRA rank, increased for potentially better quality with real dataset\n",
        "    lora_alpha=64, # LoRA alpha\n",
        "    target_modules=None,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"UNet with LoRA type: {type(unet_lora)}\")\n",
        "unet_lora.print_trainable_parameters()\n",
        "\n",
        "output_dir = \"./tpu_diffusion_finetuned_oldbooks\"\n",
        "training_args = DiffusionTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    # num_train_epochs=3, # Adjust epochs based on dataset size and desired fine-tuning\n",
        "    max_steps = 200,\n",
        "    per_device_train_batch_size=2, # Adjust based on TPU memory (512x512 images are larger)\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=50, # Adjust based on total steps\n",
        "    logging_steps=20,\n",
        "    tpu_num_cores=xm.xrt_world_size() if IS_TPU_AVAILABLE and xm.xrt_world_size() is not None else None,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MT96hdeDyHGq",
      "metadata": {
        "id": "MT96hdeDyHGq"
      },
      "source": [
        "## 6. Fine-tuning Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lpiw9BSsyHGr",
      "metadata": {
        "id": "lpiw9BSsyHGr"
      },
      "source": [
        "This function will be executed on each TPU core. It sets up the `DiffusionTrainer` and starts the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wNlKWBGLyHGr",
      "metadata": {
        "id": "wNlKWBGLyHGr"
      },
      "outputs": [],
      "source": [
        "def training_function(index, unet_lora_model, tokenizer_obj, vae_obj, scheduler_obj,\n",
        "                        train_ds, collate_fn_obj, training_args_obj):\n",
        "    \"\"\"The main training function to be executed on each TPU core.\"\"\"\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Process {index} using device: {device}\")\n",
        "\n",
        "    unet_lora_model.to(device)\n",
        "    vae_obj.to(device)\n",
        "    # text_encoder is part of unet_lora_model (FastDiffusionModel attaches it)\n",
        "    text_encoder_obj_local = unet_lora_model.text_encoder\n",
        "    if text_encoder_obj_local: text_encoder_obj_local.to(device)\n",
        "    else:\n",
        "        # Fallback if not attached, though FastDiffusionModel should handle this\n",
        "        # This indicates a potential issue if text_encoder_obj_local is None here\n",
        "        print(f\"Warning: Process {index} could not find text_encoder on unet_lora_model. Using global text_encoder.\")\n",
        "        text_encoder_obj_local = text_encoder # Global text_encoder from cell 3\n",
        "        text_encoder_obj_local.to(device)\n",
        "\n",
        "    trainer = DiffusionTrainer(\n",
        "        model=unet_lora_model,\n",
        "        args=training_args_obj,\n",
        "        train_dataset=train_ds,\n",
        "        tokenizer=tokenizer_obj,\n",
        "        data_collator=collate_fn_obj,\n",
        "        text_encoder=text_encoder_obj_local,\n",
        "        vae=vae_obj,\n",
        "        scheduler=scheduler_obj,\n",
        "    )\n",
        "\n",
        "    print(f\"Process {index}: Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    xm.rendezvous(f\"process_train_done_{index}\")\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(f\"Process {index} (master): Saving LoRA model adapters...\")\n",
        "        save_lora_dir = os.path.join(training_args_obj.output_dir, \"lora_adapters\")\n",
        "        unet_lora_model.save_pretrained(save_lora_dir)\n",
        "        print(f\"Process {index} (master): LoRA adapters saved to {save_lora_dir}\")\n",
        "\n",
        "    xm.rendezvous(f\"process_save_done_{index}\")\n",
        "    print(f\"Process {index}: Training and saving finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_IlvlUlyHGr",
      "metadata": {
        "id": "H_IlvlUlyHGr"
      },
      "source": [
        "## 7. Launch Distributed Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JYZ6h06HyHGr",
      "metadata": {
        "id": "JYZ6h06HyHGr"
      },
      "source": [
        "We use `DiffusionTrainer.launch_distributed` (which is a static method calling `xmp.spawn`) to start the training on all TPU cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JPgirRzMyHGr",
      "metadata": {
        "id": "JPgirRzMyHGr"
      },
      "outputs": [],
      "source": [
        "if IS_TPU_AVAILABLE and len(processed_dataset) > 0:\n",
        "    args_for_spawn = (unet_lora, tokenizer, vae, scheduler,\n",
        "                        processed_dataset, collate_fn, training_args)\n",
        "\n",
        "    print(\"Launching distributed training on TPUs...\")\n",
        "    DiffusionTrainer.launch_distributed(training_function, args=args_for_spawn)\n",
        "    print(\"Distributed training finished.\")\n",
        "elif not IS_TPU_AVAILABLE:\n",
        "    print(\"Skipping distributed training as TPU is not available or configured.\")\n",
        "else:\n",
        "    print(\"Skipping distributed training as processed dataset is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "krCxdH4NyHGr",
      "metadata": {
        "id": "krCxdH4NyHGr"
      },
      "source": [
        "## 8. Inference with Fine-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bFTNwvX4yHGr",
      "metadata": {
        "id": "bFTNwvX4yHGr"
      },
      "source": [
        "After training, the LoRA adapters are saved. Now, we'll load the original base model and apply these fine-tuned adapters to perform inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EfEXiMLWyHGr",
      "metadata": {
        "id": "EfEXiMLWyHGr"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_images(images, prompts, cols=2):\n",
        "    rows = (len(images) + cols - 1) // cols\n",
        "    plt.figure(figsize=(15, 5 * rows))\n",
        "    for i, (image, prompt) in enumerate(zip(images, prompts)):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Prompt: {prompt}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if IS_TPU_AVAILABLE or not IS_TPU_AVAILABLE: # Allow inference test even if training skipped for local CPU/GPU test\n",
        "    lora_adapter_path = os.path.join(output_dir, \"lora_adapters\")\n",
        "\n",
        "    if not os.path.exists(lora_adapter_path):\n",
        "        print(f\"LoRA adapters not found at {lora_adapter_path}. Ensure training completed or path is correct. Skipping inference.\")\n",
        "    else:\n",
        "        print(\"Loading base model for inference...\")\n",
        "        device_for_inference = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if IS_TPU_AVAILABLE and xm is not None : device_for_inference = xm.xla_device()\n",
        "\n",
        "        base_pipeline_for_inference = StableDiffusionPipeline.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16 if IS_TPU_AVAILABLE else torch.float16,\n",
        "        )\n",
        "        base_pipeline_for_inference.to(device_for_inference)\n",
        "        unet_for_inference = base_pipeline_for_inference.unet\n",
        "\n",
        "        print(f\"Loading LoRA adapters from {lora_adapter_path} into UNet...\")\n",
        "        unet_with_lora = PeftModel.from_pretrained(unet_for_inference, lora_adapter_path)\n",
        "        # Ensure the merged model is on the correct device, especially if base UNet was on CPU and LoRA layers were loaded.\n",
        "        unet_with_lora = unet_with_lora.to(device_for_inference)\n",
        "        unet_with_lora.eval()\n",
        "\n",
        "        base_pipeline_for_inference.unet = unet_with_lora\n",
        "\n",
        "        print(\"Generating images with fine-tuned LoRA model...\")\n",
        "        prompts = [\n",
        "            \"A man standing on top of a tank holding an american flag.\",\n",
        "        ]\n",
        "        generated_images = []\n",
        "\n",
        "        # Set a seed for reproducibility if desired\n",
        "        # generator = torch.Generator(device=device_for_inference).manual_seed(42)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for prompt in prompts:\n",
        "                image = base_pipeline_for_inference(prompt, num_inference_steps=30, height=image_size, width=image_size).images[0]\n",
        "                generated_images.append(image)\n",
        "\n",
        "        print(\"Displaying generated images...\")\n",
        "        display_images(generated_images, prompts)\n",
        "else:\n",
        "    # This case (IS_TPU_AVAILABLE being False after the initial check) should ideally not be hit if notebook is for TPU\n",
        "    print(\"Skipping inference as TPU was not available and training was likely not performed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
