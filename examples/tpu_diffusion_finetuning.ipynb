{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HXDgQ_gayHGl",
      "metadata": {
        "id": "HXDgQ_gayHGl"
      },
      "source": [
        "# Fine-tuning Stable Diffusion with LoRA on TPUs using Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XnUSUdTqyHGm",
      "metadata": {
        "id": "XnUSUdTqyHGm"
      },
      "source": [
        "This notebook demonstrates how to fine-tune a pre-trained Stable Diffusion model on a specific task using LoRA (Low-Rank Adaptation) with Unsloth's `FastDiffusionModel` and `DiffusionTrainer` on a TPU runtime.\n",
        "\n",
        "**Key Steps:**\n",
        "1. Setup: Install libraries and configure TPU environment.\n",
        "2. Load Model: Use `FastDiffusionModel` to load a Stable Diffusion pipeline.\n",
        "3. Prepare Dataset: Create or load an image-caption dataset and preprocess it.\n",
        "4. Configure LoRA: Apply LoRA adapters to the UNet component of the diffusion model.\n",
        "5. Define Training Function: Create a function for the distributed training loop.\n",
        "6. Launch Distributed Training: Use `DiffusionTrainer.launch_distributed` to train on multiple TPU cores.\n",
        "7. Inference: Load the fine-tuned LoRA adapters and generate images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zuiqpc5yHGn",
      "metadata": {
        "id": "2zuiqpc5yHGn"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2XjgbrwLyHGn",
      "metadata": {
        "id": "2XjgbrwLyHGn"
      },
      "source": [
        "**Ensure TPU Runtime:**\n",
        "If you are using Google Colab, make sure to select a TPU runtime:\n",
        "1. Go to `Runtime` -> `Change runtime type`.\n",
        "2. Select `TPU` from the `Hardware accelerator` dropdown.\n",
        "\n",
        "**Install Libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T1Vde2sHyHGo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T1Vde2sHyHGo",
        "outputId": "6b2004cc-6f27-4b7e-d611-d89885933e6f"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --no-deps diffusers Pillow transformers bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo sentencepiece protobuf datasets huggingface_hub hf_transfer multiprocess xxhash dill\n",
        "!pip install --no-deps \"unsloth[tpu] @ git+https://github.com/Sweaterdog/unsloth.git\" # Install Unsloth with TPU extras from Sweaterdog's fork\n",
        "!pip install --no-deps unsloth_zoo triton==2.2.0 # Pinned Triton version\n",
        "!pip install --force-reinstall torch_xla\n",
        "\n",
        "# Verify torch_xla installation (optional)\n",
        "try:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    print(\"Successfully imported torch_xla.core.xla_model as xm\")\n",
        "    print(f\"Default XLA device: {xm.xla_device()}\")\n",
        "    print(f\"XLA world size: {xm.xrt_world_size()}\")\n",
        "except ImportError:\n",
        "    print(\"torch_xla not found. Please ensure it's installed correctly for TPU usage.\")\n",
        "    print(\"Attempting to install torch_xla\")\n",
        "    !pip install --force-reinstall torch_xla"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_restart_runtime_cell",
      "metadata": {
        "id": "new_restart_runtime_cell"
      },
      "source": [
        "**IMPORTANT: Restart Your Runtime!**\n",
        "\n",
        "After running the package installation cell above, you **MUST** restart the runtime for the changes to the `unsloth` package (especially for TPU device detection) to be loaded correctly.\n",
        "\n",
        "**In Google Colab:**\n",
        "1. Go to the menu: `Runtime` -> `Restart session` (or `Restart runtime`).\n",
        "2. Wait for the session to restart.\n",
        "3. Then, continue running the cells from the **\"Import Libraries:\"** section downwards.\n",
        "\n",
        "Failure to restart may result in a `NotImplementedError` related to device detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AsFaD0gXyHGo",
      "metadata": {
        "id": "AsFaD0gXyHGo"
      },
      "source": [
        "**Import Libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "jGhUOqrVyHGo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jGhUOqrVyHGo",
        "outputId": "a6a5aa70-0fa3-4429-c6df-734161b2310d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1937100216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastDiffusionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusionTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusionTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m[0mcore\u001b[0m\u001b[0;34m.\u001b[0m[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.\"\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=[0m \u001b[0mget_device_type\u001b[0m[0;34m(\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Reduce VRAM usage by reducing fragmentation\u001b[0m\u001b[0;34m\u001b[0m[0;34m\u001b[0m[0m
",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs.\"\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;32mpass\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=[0m \u001b[0mget_device_type\u001b[0m[0;34m(\u001b[0m[0;34m)\u001b[0m[0;34m\u001b[0m[0;34m\u001b[0m[0m
",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs, Intel GPUs, or TPUs."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset # Updated import for load_dataset\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import random\n",
        "\n",
        "from unsloth import FastDiffusionModel, DiffusionTrainer, DiffusionTrainingArguments\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pjU05sRyyHGp",
      "metadata": {
        "id": "pjU05sRyyHGp"
      },
      "source": [
        "## 2. Initialize TPU Distributed Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l6AbjlrJyHGp",
      "metadata": {
        "id": "l6AbjlrJyHGp"
      },
      "source": [
        "TPUs excel at distributed training, where multiple cores work together. Unsloth's `DiffusionTrainer` provides a `launch_distributed` method that simplifies this process. It uses `torch_xla.distributed.xla_multiprocessing.spawn` (xmp.spawn) to run the training function on all available TPU cores.\n",
        "\n",
        "The `train_fn` we define later will encapsulate the training logic for a single process. `launch_distributed` will handle spawning this function across all TPU cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NPQfgYiLyHGp",
      "metadata": {
        "id": "NPQfgYiLyHGp"
      },
      "outputs": [],
      "source": [
        "def check_tpu_availability():\n",
        "    if 'COLAB_TPU_ADDR' in os.environ or 'XRT_TPU_CONFIG' in os.environ:\n",
        "        print(f\"TPU available.\")\n",
        "        try:\n",
        "            print(f\"Number of XLA devices: {xm.xrt_world_size()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not get XLA world size: {e}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"TPU not detected. This notebook is designed for TPU runtimes.\")\n",
        "        print(\"If on Colab, ensure Runtime > Change runtime type > TPU is selected.\")\n",
        "        return False\n",
        "\n",
        "IS_TPU_AVAILABLE = check_tpu_availability()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1xkeVPxRyHGp",
      "metadata": {
        "id": "1xkeVPxRyHGp"
      },
      "source": [
        "## 3. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffFWK6pnyHGp",
      "metadata": {
        "id": "ffFWK6pnyHGp"
      },
      "source": [
        "We'll use `FastDiffusionModel.from_pretrained` to load `stable-diffusion-v1-5/stable-diffusion-v1-5`. This method returns the UNet (as the main `model` object), the tokenizer, and the full diffusers pipeline. The UNet will have references to other components like VAE, text encoder, and scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YGa7GAzWyHGp",
      "metadata": {
        "id": "YGa7GAzWyHGp"
      },
      "outputs": [],
      "source": [
        "model_name = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        "\n",
        "unet, tokenizer, pipeline = FastDiffusionModel.from_pretrained(\n",
        "    model_name_or_path=model_name,\n",
        "    torch_dtype=torch.bfloat16, # bfloat16 is recommended for TPUs\n",
        ")\n",
        "\n",
        "# Access components (they are also attributes of the unet object itself)\n",
        "vae = unet.vae\n",
        "text_encoder = unet.text_encoder\n",
        "scheduler = unet.scheduler\n",
        "\n",
        "print(f\"UNet type: {type(unet)}\")\n",
        "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
        "print(f\"Pipeline type: {type(pipeline)}\")\n",
        "print(f\"VAE type: {type(vae)}\")\n",
        "print(f\"Text Encoder type: {type(text_encoder)}\")\n",
        "print(f\"Scheduler type: {type(scheduler)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48Xdy8zyHGq",
      "metadata": {
        "id": "a48Xdy8zyHGq"
      },
      "source": [
        "## 4. Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4y1OY2yHGq",
      "metadata": {
        "id": "da4y1OY2yHGq"
      },
      "source": [
        "For fine-tuning, we need a dataset of images and corresponding text captions. We will use the `gigant/oldbookillustrations` dataset from Hugging Face Hub. The `DiffusionTrainer` expects inputs in a dictionary format, typically including `pixel_values` (for images) and `input_ids` (for tokenized captions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AyaSnjasyHGq",
      "metadata": {
        "id": "AyaSnjasyHGq"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"gigant/oldbookillustrations\"\n",
        "image_column = \"1600px\" # Default for this dataset\n",
        "caption_column = \"info_alt\" # Default for this dataset, sometimes 'text'\n",
        "\n",
        "image_size = 1600  # runwayml/stable-diffusion-v1-5 uses 512x512 images\n",
        "\n",
        "# Load a subset of the dataset\n",
        "try:\n",
        "    full_dataset = load_dataset(dataset_name)\n",
        "    # Select a small subset for the demo from the 'train' split\n",
        "    # Ensure 'train' split exists, otherwise adapt (e.g. if only 'default' split)\n",
        "    split_name = 'train' if 'train' in full_dataset else list(full_dataset.keys())[0]\n",
        "    hf_dataset = full_dataset[split_name].shuffle(seed=42).select(range(1000))\n",
        "    print(f\"Loaded {len(hf_dataset)} samples from {dataset_name} ({split_name} split).\")\n",
        "    print(f\"Dataset features: {hf_dataset.features}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset {dataset_name}: {e}\")\n",
        "    print(\"Using a dummy dataset as a fallback.\")\n",
        "    num_samples = 64\n",
        "    dataset_dict = {\"image\": [], \"caption\": []}\n",
        "    for i in range(num_samples):\n",
        "        img = Image.new('RGB', (image_size, image_size), color=random.choice([\"red\", \"blue\", \"green\", \"yellow\"]))\n",
        "        dataset_dict[\"image\"].append(img)\n",
        "        dataset_dict[\"caption\"].append(f\"A dummy illustration of a colored shape number {i}\")\n",
        "    hf_dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Preprocessing functions\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "def preprocess_dataset(examples):\n",
        "    images = [image_transforms(image.convert(\"RGB\")) for image in examples[image_column]]\n",
        "    captions = examples[caption_column]\n",
        "\n",
        "    max_len = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length else 77\n",
        "    inputs = tokenizer(\n",
        "        captions, max_length=max_len, padding=\"max_length\",\n",
        "        truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\"pixel_values\": images, \"input_ids\": inputs.input_ids}\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_dataset = hf_dataset.map(\n",
        "    function=preprocess_dataset,\n",
        "    batched=True,\n",
        "    remove_columns=[col for col in hf_dataset.column_names if col not in [\"pixel_values\", \"input_ids\"]], # Keep only needed\n",
        ")\n",
        "\n",
        "# Define the data collator (stacks tensors)\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "print(f\"Processed dataset features: {processed_dataset.features}\")\n",
        "if len(processed_dataset) > 0:\n",
        "    print(f\"Example pixel_values shape: {processed_dataset[0]['pixel_values'].shape}\")\n",
        "    print(f\"Example input_ids shape: {processed_dataset[0]['input_ids'].shape}\")\n",
        "else:\n",
        "    print(\"Processed dataset is empty, check dataset loading and preprocessing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0dj_17yHGq",
      "metadata": {
        "id": "0a0dj_17yHGq"
      },
      "source": [
        "## 5. Configure LoRA and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3vzqZ_k2yHGq",
      "metadata": {
        "id": "3vzqZ_k2yHGq"
      },
      "source": [
        "Now, we'll apply LoRA to the UNet. `FastDiffusionModel.get_peft_model` (which is available on the UNet object itself after loading with `FastDiffusionModel`) helps with this. Then, we define `DiffusionTrainingArguments` and prepare for trainer instantiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t49o3svyyHGq",
      "metadata": {
        "id": "t49o3svyyHGq"
      },
      "outputs": [],
      "source": [
        "# Apply LoRA to the UNet\n",
        "unet_lora = FastDiffusionModel.get_peft_model(\n",
        "    unet,\n",
        "    r=64, # LoRA rank, increased for potentially better quality with real dataset\n",
        "    lora_alpha=64, # LoRA alpha\n",
        "    target_modules=None,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"UNet with LoRA type: {type(unet_lora)}\")\n",
        "unet_lora.print_trainable_parameters()\n",
        "\n",
        "output_dir = \"./tpu_diffusion_finetuned_oldbooks\"\n",
        "training_args = DiffusionTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    # num_train_epochs=3, # Adjust epochs based on dataset size and desired fine-tuning\n",
        "    max_steps = 200,\n",
        "    per_device_train_batch_size=2, # Adjust based on TPU memory (512x512 images are larger)\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=50, # Adjust based on total steps\n",
        "    logging_steps=20,\n",
        "    tpu_num_cores=xm.xrt_world_size() if IS_TPU_AVAILABLE and xm.xrt_world_size() is not None else None,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MT96hdeDyHGq",
      "metadata": {
        "id": "MT96hdeDyHGq"
      },
      "source": [
        "## 6. Fine-tuning Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lpiw9BSsyHGr",
      "metadata": {
        "id": "lpiw9BSsyHGr"
      },
      "source": [
        "This function will be executed on each TPU core. It sets up the `DiffusionTrainer` and starts the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wNlKWBGLyHGr",
      "metadata": {
        "id": "wNlKWBGLyHGr"
      },
      "outputs": [],
      "source": [
        "def training_function(index, unet_lora_model, tokenizer_obj, vae_obj, scheduler_obj,\n",
        "                        train_ds, collate_fn_obj, training_args_obj):\n",
        "    \"\"\"The main training function to be executed on each TPU core.\"\"\"\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Process {index} using device: {device}\")\n",
        "\n",
        "    unet_lora_model.to(device)\n",
        "    vae_obj.to(device)\n",
        "    # text_encoder is part of unet_lora_model (FastDiffusionModel attaches it)\n",
        "    text_encoder_obj_local = unet_lora_model.text_encoder\n",
        "    if text_encoder_obj_local: text_encoder_obj_local.to(device)\n",
        "    else:\n",
        "        # Fallback if not attached, though FastDiffusionModel should handle this\n",
        "        # This indicates a potential issue if text_encoder_obj_local is None here\n",
        "        print(f\"Warning: Process {index} could not find text_encoder on unet_lora_model. Using global text_encoder.\")\n",
        "        text_encoder_obj_local = text_encoder # Global text_encoder from cell 3\n",
        "        text_encoder_obj_local.to(device)\n",
        "\n",
        "    trainer = DiffusionTrainer(\n",
        "        model=unet_lora_model,\n",
        "        args=training_args_obj,\n",
        "        train_dataset=train_ds,\n",
        "        tokenizer=tokenizer_obj,\n",
        "        data_collator=collate_fn_obj,\n",
        "        text_encoder=text_encoder_obj_local,\n",
        "        vae=vae_obj,\n",
        "        scheduler=scheduler_obj,\n",
        "    )\n",
        "\n",
        "    print(f\"Process {index}: Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    xm.rendezvous(f\"process_train_done_{index}\")\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(f\"Process {index} (master): Saving LoRA model adapters...\")\n",
        "        save_lora_dir = os.path.join(training_args_obj.output_dir, \"lora_adapters\")\n",
        "        unet_lora_model.save_pretrained(save_lora_dir)\n",
        "        print(f\"Process {index} (master): LoRA adapters saved to {save_lora_dir}\")\n",
        "\n",
        "    xm.rendezvous(f\"process_save_done_{index}\")\n",
        "    print(f\"Process {index}: Training and saving finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_IlvlUlyHGr",
      "metadata": {
        "id": "H_IlvlUlyHGr"
      },
      "source": [
        "## 7. Launch Distributed Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JYZ6h06HyHGr",
      "metadata": {
        "id": "JYZ6h06HyHGr"
      },
      "source": [
        "We use `DiffusionTrainer.launch_distributed` (which is a static method calling `xmp.spawn`) to start the training on all TPU cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JPgirRzMyHGr",
      "metadata": {
        "id": "JPgirRzMyHGr"
      },
      "outputs": [],
      "source": [
        "if IS_TPU_AVAILABLE and len(processed_dataset) > 0:\n",
        "    args_for_spawn = (unet_lora, tokenizer, vae, scheduler,\n",
        "                        processed_dataset, collate_fn, training_args)\n",
        "\n",
        "    print(\"Launching distributed training on TPUs...\")\n",
        "    DiffusionTrainer.launch_distributed(training_function, args=args_for_spawn)\n",
        "    print(\"Distributed training finished.\")\n",
        "elif not IS_TPU_AVAILABLE:\n",
        "    print(\"Skipping distributed training as TPU is not available or configured.\")\n",
        "else:\n",
        "    print(\"Skipping distributed training as processed dataset is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "krCxdH4NyHGr",
      "metadata": {
        "id": "krCxdH4NyHGr"
      },
      "source": [
        "## 8. Inference with Fine-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bFTNwvX4yHGr",
      "metadata": {
        "id": "bFTNwvX4yHGr"
      },
      "source": [
        "After training, the LoRA adapters are saved. Now, we'll load the original base model and apply these fine-tuned adapters to perform inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EfEXiMLWyHGr",
      "metadata": {
        "id": "EfEXiMLWyHGr"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_images(images, prompts, cols=2):\n",
        "    rows = (len(images) + cols - 1) // cols\n",
        "    plt.figure(figsize=(15, 5 * rows))\n",
        "    for i, (image, prompt) in enumerate(zip(images, prompts)):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Prompt: {prompt}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if IS_TPU_AVAILABLE or not IS_TPU_AVAILABLE: # Allow inference test even if training skipped for local CPU/GPU test\n",
        "    lora_adapter_path = os.path.join(output_dir, \"lora_adapters\")\n",
        "\n",
        "    if not os.path.exists(lora_adapter_path):\n",
        "        print(f\"LoRA adapters not found at {lora_adapter_path}. Ensure training completed or path is correct. Skipping inference.\")\n",
        "    else:\n",
        "        print(\"Loading base model for inference...\")\n",
        "        device_for_inference = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if IS_TPU_AVAILABLE and xm is not None : device_for_inference = xm.xla_device()\n",
        "\n",
        "        base_pipeline_for_inference = StableDiffusionPipeline.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16 if IS_TPU_AVAILABLE else torch.float16,\n",
        "        )\n",
        "        base_pipeline_for_inference.to(device_for_inference)\n",
        "        unet_for_inference = base_pipeline_for_inference.unet\n",
        "\n",
        "        print(f\"Loading LoRA adapters from {lora_adapter_path} into UNet...\")\n",
        "        unet_with_lora = PeftModel.from_pretrained(unet_for_inference, lora_adapter_path)\n",
        "        # Ensure the merged model is on the correct device, especially if base UNet was on CPU and LoRA layers were loaded.\n",
        "        unet_with_lora = unet_with_lora.to(device_for_inference)\n",
        "        unet_with_lora.eval()\n",
        "\n",
        "        base_pipeline_for_inference.unet = unet_with_lora\n",
        "\n",
        "        print(\"Generating images with fine-tuned LoRA model...\")\n",
        "        prompts = [\n",
        "            \"A man standing on top of a tank holding an american flag.\",\n",
        "        ]\n",
        "        generated_images = []\n",
        "\n",
        "        # Set a seed for reproducibility if desired\n",
        "        # generator = torch.Generator(device=device_for_inference).manual_seed(42)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for prompt in prompts:\n",
        "                image = base_pipeline_for_inference(prompt, num_inference_steps=30, height=image_size, width=image_size).images[0]\n",
        "                generated_images.append(image)\n",
        "\n",
        "        print(\"Displaying generated images...\")\n",
        "        display_images(generated_images, prompts)\n",
        "else:\n",
        "    # This case (IS_TPU_AVAILABLE being False after the initial check) should ideally not be hit if notebook is for TPU\n",
        "    print(\"Skipping inference as TPU was not available and training was likely not performed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
