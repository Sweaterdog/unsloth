{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Stable Diffusion with LoRA on TPUs using Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to fine-tune a pre-trained Stable Diffusion model on a specific task using LoRA (Low-Rank Adaptation) with Unsloth's `FastDiffusionModel` and `DiffusionTrainer` on a TPU runtime.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Setup: Install libraries and configure TPU environment.\n",
    "2. Load Model: Use `FastDiffusionModel` to load a Stable Diffusion pipeline.\n",
    "3. Prepare Dataset: Create or load an image-caption dataset and preprocess it.\n",
    "4. Configure LoRA: Apply LoRA adapters to the UNet component of the diffusion model.\n",
    "5. Define Training Function: Create a function for the distributed training loop.\n",
    "6. Launch Distributed Training: Use `DiffusionTrainer.launch_distributed` to train on multiple TPU cores.\n",
    "7. Inference: Load the fine-tuned LoRA adapters and generate images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure TPU Runtime:**\n",
    "If you are using Google Colab, make sure to select a TPU runtime:\n",
    "1. Go to `Runtime` -> `Change runtime type`.\n",
    "2. Select `TPU` from the `Hardware accelerator` dropdown.\n",
    "\n",
    "**Install Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install \"unsloth[tpu] @ git+https://github.com/unslothai/unsloth.git\" # Install unsloth with TPU extras\n",
    "!pip install diffusers transformers accelerate datasets Pillow\n",
    "\n",
    "# It's crucial that torch_xla is installed. Unsloth's TPU extras should handle this.\n",
    "# If you encounter issues, you might need to manually install a compatible torch_xla version:\n",
    "# import os\n",
    "# if os.environ.get('COLAB_TPU_ADDR'):\n",
    "#   !pip install torch_xla cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.1-cp310-cp310-linux_x86_64.whl\n",
    "# else:\n",
    "#   print('Not running on a Colab TPU')\n",
    "\n",
    "# Verify torch_xla installation (optional)\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    print(f\"torch_xla version: {xm.__version__}\")\n",
    "    print(f\"Default XLA device: {xm.xla_device()}\")\n",
    "    print(f\"XLA world size: {xm.xrt_world_size()}\")\n",
    "except ImportError:\n",
    "    print(\"torch_xla not found. Please ensure it's installed correctly for TPU usage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from unsloth import FastDiffusionModel, DiffusionTrainer, DiffusionTrainingArguments\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize TPU Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPUs excel at distributed training, where multiple cores work together. Unsloth's `DiffusionTrainer` provides a `launch_distributed` method that simplifies this process. It uses `torch_xla.distributed.xla_multiprocessing.spawn` (xmp.spawn) to run the training function on all available TPU cores.\n",
    "\n",
    "The `train_fn` we define later will encapsulate the training logic for a single process. `launch_distributed` will handle spawning this function across all TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tpu_availability():\n",
    "    if 'COLAB_TPU_ADDR' in os.environ:\n",
    "        print(f\"TPU available. Address: {os.environ['COLAB_TPU_ADDR']}\")\n",
    "        print(f\"Number of XLA devices: {xm.xrt_world_size()}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"TPU not detected. This notebook is designed for TPU runtimes.\")\n",
    "        print(\"If on Colab, ensure Runtime > Change runtime type > TPU is selected.\")\n",
    "        # For local testing without a real TPU, one might mock xm, but launch_distributed won't work.\n",
    "        return False\n",
    "\n",
    "IS_TPU_AVAILABLE = check_tpu_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `FastDiffusionModel.from_pretrained` to load a small, pre-trained Stable Diffusion model. This method returns the UNet (as the main `model` object), the tokenizer, and the full diffusers pipeline. The UNet will have references to other components like VAE, text encoder, and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a tiny model for quick demonstration\n",
    "model_name = \"hf-internal-testing/tiny-stable-diffusion-pipe\"\n",
    "# For a more standard model, you might use:\n",
    "# model_name = \"runwayml/stable-diffusion-v1-5\" \n",
    "# model_name = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "unet, tokenizer, pipeline = FastDiffusionModel.from_pretrained(\n",
    "    model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16, # bfloat16 is recommended for TPUs\n",
    ")\n",
    "\n",
    "# Access components (they are also attributes of the unet object itself)\n",
    "vae = unet.vae\n",
    "text_encoder = unet.text_encoder\n",
    "scheduler = unet.scheduler\n",
    "\n",
    "print(f\"UNet type: {type(unet)}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")\n",
    "print(f\"Pipeline type: {type(pipeline)}\")\n",
    "print(f\"VAE type: {type(vae)}\")\n",
    "print(f\"Text Encoder type: {type(text_encoder)}\")\n",
    "print(f\"Scheduler type: {type(scheduler)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning, we need a dataset of images and corresponding text captions. The `DiffusionTrainer` expects inputs in a dictionary format, typically including `pixel_values` (for images) and `input_ids` (for tokenized captions).\n",
    "\n",
    "Here, we'll create a small dummy dataset for demonstration purposes. In a real scenario, you would load your own dataset (e.g., from Hugging Face Hub or local files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy dataset generation\n",
    "num_samples = 64 # Should be divisible by number of TPU cores * batch_size for simplicity\n",
    "image_size = 64  # Using very small images for this tiny model example (tiny-sd outputs 128x128 by default)\n",
    "                   # For runwayml/stable-diffusion-v1-5 use 512\n",
    "                   # For stabilityai/stable-diffusion-2-1-base use 768\n",
    "\n",
    "dummy_captions = [\n",
    "    \"A photo of a red square\",\n",
    "    \"A drawing of a blue circle\",\n",
    "    \"An image of a green triangle\",\n",
    "    \"A painting of a yellow star\"\n",
    "]\n",
    "\n",
    "def generate_dummy_image_data(width, height, color):\n",
    "    img = Image.new('RGB', (width, height), color=color)\n",
    "    return img\n",
    "\n",
    "dataset_dict = {\"image\": [], \"text\": []}\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "shapes = [\"square\", \"circle\", \"triangle\", \"star\"]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    color = random.choice(colors)\n",
    "    shape = random.choice(shapes)\n",
    "    # img_data = generate_dummy_image_data(image_size, image_size, color)\n",
    "    # For the tiny model, it's better to use images it might have seen (like cats, dogs)\n",
    "    # Since we can't download easily, we'll stick to simple color images for pure technical demo.\n",
    "    # If you use a real model, use real images.\n",
    "    img = Image.new('RGB', (image_size, image_size), color=random.choice(colors))\n",
    "    dataset_dict[\"image\"].append(img)\n",
    "    dataset_dict[\"text\"].append(f\"A photo of a {color} {shape}\")\n",
    "\n",
    "hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Preprocessing functions\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "def preprocess_dataset(examples):\n",
    "    images = [image_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    captions = examples[\"text\"]\n",
    "    \n",
    "    # Tokenize captions\n",
    "    # Max length should be based on tokenizer's model_max_length\n",
    "    # For tiny-sd, it might be small. For SD 1.5/2.1, it's usually 77.\n",
    "    max_len = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') else 77\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=max_len, padding=\"max_length\", \n",
    "        truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\"pixel_values\": images, \"input_ids\": inputs.input_ids}\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "# This is done on the main process before distributing data\n",
    "processed_dataset = hf_dataset.map(\n",
    "    function=preprocess_dataset, \n",
    "    batched=True, \n",
    "    remove_columns=[\"image\", \"text\"]\n",
    ")\n",
    "\n",
    "# Define the data collator (stacks tensors)\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "print(f\"Processed dataset features: {processed_dataset.features}\")\n",
    "print(f\"Example pixel_values shape: {processed_dataset[0]['pixel_values'].shape}\")\n",
    "print(f\"Example input_ids shape: {processed_dataset[0]['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll apply LoRA to the UNet. `FastDiffusionModel.get_peft_model` (which is available on the UNet object itself after loading with `FastDiffusionModel`) helps with this. Then, we define `DiffusionTrainingArguments` and instantiate the `DiffusionTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the UNet\n",
    "# The 'unet' object obtained from FastDiffusionModel.from_pretrained is the UNet itself.\n",
    "unet_lora = FastDiffusionModel.get_peft_model(\n",
    "    unet, # Pass the UNet model\n",
    "    r=16, # LoRA rank\n",
    "    lora_alpha=32, # LoRA alpha\n",
    "    target_modules=None, # Let Unsloth determine default UNet targets or specify manually\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True, # Recommended for large models\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"UNet with LoRA type: {type(unet_lora)}\")\n",
    "unet_lora.print_trainable_parameters() # PEFT model utility\n",
    "\n",
    "# Define Training Arguments\n",
    "output_dir = \"./tpu_diffusion_finetuned_lora\"\n",
    "training_args = DiffusionTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2, # Small number of epochs for demo\n",
    "    per_device_train_batch_size=2, # Adjust based on TPU memory and image size\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=10,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=50, # How often to save checkpoints (if desired, not critical for LoRA only)\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2, # If using torch Dataloader\n",
    "    tpu_num_cores=xm.xrt_world_size() if IS_TPU_AVAILABLE else None, # Specify number of TPU cores\n",
    "    # bf16=True, # Handled by dtype in from_pretrained for model, trainer will use what model uses\n",
    "    report_to=\"none\", # Disable wandb/tensorboard for this demo\n",
    "    remove_unused_columns=False, # Ensure all columns needed by compute_loss are kept\n",
    ")\n",
    "\n",
    "# Note: DiffusionTrainer is instantiated inside the train_fn for distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be executed on each TPU core. It sets up the `DiffusionTrainer` and starts the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(index, unet_lora_model, tokenizer_obj, vae_obj, scheduler_obj, \n",
    "                        train_ds, collate_fn_obj, training_args_obj):\n",
    "    \"\"\"The main training function to be executed on each TPU core.\"\"\"\n",
    "    \n",
    "    # Set the XLA device for this process\n",
    "    device = xm.xla_device()\n",
    "    print(f\"Process {index} using device: {device}\")\n",
    "\n",
    "    # Move models to the current TPU core's device\n",
    "    # Although _wrap_model in trainer does this, it's good practice if using components separately\n",
    "    unet_lora_model.to(device)\n",
    "    vae_obj.to(device)\n",
    "    text_encoder_obj = unet_lora_model.text_encoder # text_encoder is part of unet here\n",
    "    if text_encoder_obj: text_encoder_obj.to(device)\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = DiffusionTrainer(\n",
    "        model=unet_lora_model, # This is the UNet with LoRA adapters\n",
    "        args=training_args_obj,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer_obj, # CLIP Tokenizer\n",
    "        data_collator=collate_fn_obj,\n",
    "        # Pass other pipeline components needed for compute_loss\n",
    "        text_encoder=text_encoder_obj,\n",
    "        vae=vae_obj,\n",
    "        scheduler=scheduler_obj,\n",
    "    )\n",
    "    \n",
    "    print(f\"Process {index}: Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Wait for all processes to finish training before saving\n",
    "    xm.rendezvous(f\"process_train_done_{index}\") # Barrier\n",
    "    \n",
    "    # Save the LoRA adapters (only on the master process)\n",
    "    if xm.is_master_ordinal():\n",
    "        print(f\"Process {index} (master): Saving LoRA model adapters...\")\n",
    "        # The model passed to trainer is unet_lora. save_pretrained will save LoRA adapters.\n",
    "        # If full model was trained, trainer.save_model() would save the whole UNet.\n",
    "        # For PEFT models, model.save_pretrained() saves adapters correctly.\n",
    "        save_lora_dir = os.path.join(training_args_obj.output_dir, \"lora_adapters\")\n",
    "        unet_lora_model.save_pretrained(save_lora_dir)\n",
    "        print(f\"Process {index} (master): LoRA adapters saved to {save_lora_dir}\")\n",
    "    \n",
    "    xm.rendezvous(f\"process_save_done_{index}\") # Barrier\n",
    "    print(f\"Process {index}: Training and saving finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `DiffusionTrainer.launch_distributed` (which is a static method calling `xmp.spawn`) to start the training on all TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TPU_AVAILABLE:\n",
    "    # Prepare arguments for the training function\n",
    "    # Note: Models are passed directly. xmp.spawn handles pickling/unpickling if possible,\n",
    "    # but for complex objects like models, it's often better to load them inside the spawned function\n",
    "    # or ensure they are correctly handled by XLA's multiprocessing context.\n",
    "    # For Unsloth, passing the PEFT model should be fine as it's designed with this in mind.\n",
    "    \n",
    "    # Make sure components not part of unet_lora (like original VAE, scheduler) are passed\n",
    "    # The text_encoder is part of the unet_lora object as unet_lora.text_encoder\n",
    "    args_for_spawn = (unet_lora, tokenizer, vae, scheduler, \n",
    "                        processed_dataset, collate_fn, training_args)\n",
    "    \n",
    "    print(\"Launching distributed training on TPUs...\")\n",
    "    # DiffusionTrainer.launch_distributed calls xmp.spawn\n",
    "    DiffusionTrainer.launch_distributed(training_function, args=args_for_spawn)\n",
    "    print(\"Distributed training finished.\")\n",
    "else:\n",
    "    print(\"Skipping distributed training as TPU is not available or configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference with Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the LoRA adapters are saved. Now, we'll load the original base model and apply these fine-tuned adapters to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from diffusers import StableDiffusionPipeline # For loading base pipeline for inference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(images, prompts, cols=2):\n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    plt.figure(figsize=(15, 5 * rows))\n",
    "    for i, (image, prompt) in enumerate(zip(images, prompts)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Prompt: {prompt}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if IS_TPU_AVAILABLE: # Only run inference if training was attempted\n",
    "    # Path where LoRA adapters were saved\n",
    "    lora_adapter_path = os.path.join(output_dir, \"lora_adapters\")\n",
    "\n",
    "    if not os.path.exists(lora_adapter_path):\n",
    "        print(f\"LoRA adapters not found at {lora_adapter_path}. Skipping inference.\")\n",
    "    else:\n",
    "        print(\"Loading base model for inference...\")\n",
    "        # Load the original (non-LoRA) UNet and pipeline components again\n",
    "        # Ensure this is done on CPU or a single GPU/TPU core for inference if memory is a concern\n",
    "        # For TPUs, xm.xla_device() would be the target for inference too.\n",
    "        device_for_inference = xm.xla_device() if IS_TPU_AVAILABLE else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load the base pipeline using diffusers' StableDiffusionPipeline for convenience\n",
    "        base_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.bfloat16, # Use bfloat16 for TPUs\n",
    "            # Add other components if they were modified or need specific loading\n",
    "        )\n",
    "        base_pipeline.to(device_for_inference)\n",
    "        unet_for_inference = base_pipeline.unet\n",
    "\n",
    "        print(f\"Loading LoRA adapters from {lora_adapter_path} into UNet...\")\n",
    "        # Load LoRA weights into the base UNet\n",
    "        unet_with_lora = PeftModel.from_pretrained(unet_for_inference, lora_adapter_path)\n",
    "        unet_with_lora = unet_with_lora.to(device_for_inference) # Ensure it's on device\n",
    "        unet_with_lora.eval() # Set to evaluation mode\n",
    "\n",
    "        # Replace the UNet in the pipeline with the LoRA-fused UNet\n",
    "        base_pipeline.unet = unet_with_lora\n",
    "\n",
    "        print(\"Generating images with fine-tuned LoRA model...\")\n",
    "        prompts = [\n",
    "            \"A photo of a red square\", \n",
    "            \"A drawing of a blue circle\",\n",
    "            \"A painting of a green star\", # Test generalization slightly\n",
    "            \"A photo of a yellow triangle\"\n",
    "        ]\n",
    "        generated_images = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts:\n",
    "                # For tiny-sd, output size is small. Adjust height/width if using a different base model.\n",
    "                image = base_pipeline(prompt, num_inference_steps=20, height=image_size, width=image_size).images[0]\n",
    "                generated_images.append(image)\n",
    "        \n",
    "        # Display images (requires matplotlib)\n",
    "        print(\"Displaying generated images...\")\n",
    "        display_images(generated_images, prompts)\n",
    "else:\n",
    "    print(\"Skipping inference as training was not run on TPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the example of fine-tuning a Stable Diffusion model with LoRA on TPUs using Unsloth. Remember that for real tasks, you'll need a larger, more diverse dataset and potentially more epochs and hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
